{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3s8PjB3cqbf"
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8vw-8Ubc3Mo"
   },
   "source": [
    "### QUS 1. What is Simple Linear Regression?\n",
    "- Regression is a method which is used to establish some relationship between two variables.\n",
    "- In addition we can say that it is used to establish relationship between two variables in a linear manner to predict value of dependent variable `y` with dependent of an independent variable `X`.\n",
    "- Generally it is used to To predict upcoming test results according to linear behaviour between two variables.\n",
    "- It follows the Linear equation of Y = Mx+ c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5GPAeVdgdLg"
   },
   "source": [
    "\n",
    "### QUS 2. What are the key assumptions of Simple Linear Regression?\n",
    "- Key assumptions of linear regression are as follows:\n",
    " - Linearity(X and Y should have linear relationship)\n",
    " - Independence of observations and independence of error(Both observations of X and errors occurs in a calculations y actual are independent of each observation and error respectively.)\n",
    " - Homoscandasticity(Invariants of residuals or errors are constant across values of X.)\n",
    " - Normality of errors (errors should be normal distributed)\n",
    " - No multicolliniarity(dependent variable not to be highly correlated with only one independent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMuxv_XLkBeI"
   },
   "source": [
    "### QUS 3. What does the coefficient m represent in the equation Y=mx+c?\n",
    "- The coefficient m represents the slope of the line ‚Äî it indicates how much Y changes for a one-unit change in X.\n",
    "- If the equation is:\n",
    "\n",
    "Y=3x+2\n",
    "m = 3 ‚Üí for every 1 unit increase in x, y increases by 3 units.\n",
    "\n",
    "The line goes up steeply.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkC1Ar2UmKuh"
   },
   "source": [
    "### QUS 4. What does the intercept c represent in the equation Y=mX+c?\n",
    "- Intercept C represents the value of Y when X equals to 0: It is a point where y crosses the line.\n",
    "- In case of regression it is a baseline or starting value of dependent variable where the value of independent variable is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQkHiWK8niKS"
   },
   "source": [
    "### QUS 5. How do we calculate the slope m in Simple Linear Regression?\n",
    "- We can calculate the value of `m` by using least square method of estimation.\n",
    "- m = Œ£[(x·µ¢ - xÃÑ)(y·µ¢ - »≥)] / Œ£[(x·µ¢ - xÃÑ)¬≤]\n",
    "- where:\n",
    " - xi, yi are the individual data points\n",
    "\n",
    " - x_mean is the mean of x values\n",
    "\n",
    " - y_mean is the mean of y values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1PAYQTHpZUH"
   },
   "source": [
    "### QUS 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "- The least squares method is used to find the best-fitting line through the data by minimizing the sum of the squared differences (errors) between the actual values and the predicted values.\n",
    "- These are some other characteristics of it:\n",
    " - To find the values of m (slope) and c (intercept) that make the predicted line fit the data best.\n",
    "\n",
    "  - Reduces the overall prediction error across all observations.\n",
    "\n",
    "  - Ensures the model is unbiased and efficient under certain assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_utGRZnAqVpy"
   },
   "source": [
    "### QUS 7. How is the coefficient of determination (R2) interpreted in Simple Linear Regression?\n",
    "- The coefficient of determination, denoted R¬≤, tells you how well your linear model explains the variability in the response variable (Y) using the predictor variable (X).\n",
    "\n",
    "üìà Interpretation of R¬≤:\n",
    "ùëÖ2\n",
    "=Total¬†Variation/\n",
    "Explained¬†Variation\n",
    "‚Äã=1‚àí\n",
    "SST/SSR\n",
    "\n",
    "Where:\n",
    "\n",
    "SSR = sum of squared residuals (errors)\n",
    "\n",
    "SST= total sum of squares (variation in Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjF60SmLrw7u"
   },
   "source": [
    "### QUS 8. What is Multiple Linear Regression?\n",
    "-  It is one kind of linear regression  which have multiple independant variables(predictors) to predict value of a dependant variable.\n",
    "- In others words,Multiple Linear Regression is a statistical technique used to model the relationship between one dependent variable and two or more independent variables.\n",
    "- Y = b0 + b1*X1 + b2*X2 + ... + bn*Xn + e\n",
    "- Where:\n",
    "\n",
    " - Y = dependent (target) variable\n",
    "\n",
    " -  X1, X2, ..., Xn = independent (input) variables\n",
    "\n",
    " -  b0 = intercept\n",
    "\n",
    " - b1, b2, ..., bn = coefficients (slopes)\n",
    "\n",
    " -  e = error term (residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XhedVJOtrm5"
   },
   "source": [
    "### QUS 9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "- Main difference between simple and multiple linear regression is that the simple linear regression use single independent variable to predict the behaviour of one dependent variable , where in multiple linear regression it uses multiple independent variables to predict the behaviour of 1 dependent variable.\n",
    "- In addition, We can predict effects on dependent variable with respect to multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-60GtjPuuBD"
   },
   "source": [
    "\n",
    "### QUS 10. What are the key assumptions of Multiple Linear Regression?\n",
    "\n",
    "- Key assumptions of linear regression are as follows:\n",
    " - Linearity(X and Y should have linear relationship)\n",
    " - Independence of observations and independence of error(Both observations of X and errors occurs in a calculations y actual are independent of each observation and error respectively.)\n",
    " - Homoscandasticity(Invariants of residuals or errors are constant across values of X.)\n",
    " - Normality of errors (errors should be normal distributed)\n",
    " - No multicolliniarity(dependent variable not to be highly correlated with only one independent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LW9UBF3FZFP"
   },
   "source": [
    "### QUS 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "- Heteroscedasticity refers to the condition in a regression model where the variance of the residuals (errors) is not constant.\n",
    "- Effects of it are as follows:\n",
    " - Insufficient hypothesis testing will leads to insufficient estimates.\n",
    " - Since most of the hypothesis testing are relevant on rely on assumption of Homoscendasticity that's why the results will be unreliable  and wrongly conclusive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwm75mEUJWlg"
   },
   "source": [
    "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "- By using VIF for smaller data sets and RFE for considerably large data sets to reduce highly correlated columns, We can improve multiple planar regression model with high Multicollinearity.\n",
    "- Ways to Improve the Model:\n",
    "\n",
    "  - Drop one of the variables if it‚Äôs highly correlated with another and does not provide unique information.\n",
    "\n",
    "  - Create new features that combine the information from multicollinear variables in a meaningful way using feature engineering.\n",
    "\n",
    "  - By using Regularization Techniques\n",
    "    - Shrinks coefficients and can handle multicollinearity effectively using L2 penalty.\n",
    "    - Performs feature selection by driving some coefficients to zero using L1 penalty.\n",
    "    - Combines L1 and L2 penalties for balanced regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGmVT1HbMyH2"
   },
   "source": [
    "### QUS 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
    "- Transforming categorical variables into a format that can be used in regression models,Here are the most common techniques:\n",
    " - One hot encoding\n",
    " - Label Encoding\n",
    " - Ordinal encoding\n",
    " - Nominal Encoding\n",
    " - Target guided Encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SepgnozDNyYZ"
   },
   "source": [
    "### QUS 14. What is the role of interaction terms in Multiple Linear Regression?\n",
    "- These are dome  important roles of interactive terms:\n",
    "  - Reveal Complex Relationships-Some predictors may only influence the outcome when combined with others.\n",
    "\n",
    "  - Increase Model Flexibility-Captures dependencies between variables, especially in social, economic, or biomedical data.\n",
    "\n",
    "  - Improves Predictive Power-Interaction terms can significantly increase the accuracy of predictions when such relationships exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyvJfRDqPcwn"
   },
   "source": [
    "### QUS 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "- The intercept in both Simple Linear Regression (SLR) and Multiple Linear Regression (MLR) represents the predicted value of the dependent variable when all independent variables are zero. However, its interpretation and meaningfulness can differ significantly between two of them.\n",
    "\n",
    "- In Simple Linear Regression:\n",
    "\n",
    "  The intercept is the value of the response variable when the single predictor variable is zero.\n",
    "\n",
    "  This is often easy to interpret and may be meaningful if zero is a plausible value for the predictor.\n",
    "\n",
    "  For example, if the predictor is \"temperature in Celsius,\" the intercept would be the predicted outcome at 0¬∞C.\n",
    "\n",
    "- In Multiple Linear Regression:\n",
    "\n",
    "  The intercept is the expected value of the response variable when all predictor variables are zero simultaneously.\n",
    "\n",
    "  This interpretation can be unrealistic or meaningless if zero is not a plausible value for one or more of the predictors, or if such a combination of values does not exist in the data.\n",
    "\n",
    "  For example, if predictors include age, income, and education level, it's unlikely and often not meaningful for all to be zero at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q0a2iDdS7IM"
   },
   "source": [
    "### QUS 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "- The slope in regression analysis represents the rate of change in the dependent variable (the outcome) for a one-unit increase in an independent variable, assuming all other variables are held constant (in multiple regression).\n",
    "\n",
    "- Significance of the Slope:\n",
    "\n",
    "  A positive slope means that as the independent variable increases, the dependent variable also increases.\n",
    "\n",
    "  A negative slope means that as the independent variable increases, the dependent variable decreases.\n",
    "\n",
    "  The larger the absolute value of the slope, the stronger the effect of that variable on the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLA5U_iHU15X"
   },
   "source": [
    "### QUS 17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "- The intercept in a regression model provides a baseline or starting point for understanding the relationship between the independent variables and the dependent variable.\n",
    "\n",
    "Here‚Äôs how it provides context:\n",
    "\n",
    "- Baseline Value:\n",
    "\n",
    "  - The intercept represents the predicted value of the dependent variable when all independent variables are equal to zero.\n",
    "\n",
    " - It sets the foundation upon which the effects of the independent variables (the slopes) build.\n",
    " - It allows you to interpret the impact of each independent variable relative to a known point (usually zero).provides reference point for interpretation.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev-uNpMCWp21"
   },
   "source": [
    "### QUS 18. What are the limitations of using R2 as a sole measure of model performance?\n",
    "- A high R¬≤ doesn't imply that the predictors cause changes in the dependent variable. It only reflects correlation, not causation.\n",
    "- R¬≤ always increases (or stays the same) when you add more predictors, even if they don‚Äôt improve the model meaningfully.\n",
    "- R¬≤ can vary based on the range of the dependent variable in the dataset.\n",
    "= A high R¬≤ does not mean the model predicts new data well. It only measures fit to the training data, not generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeuEKa4qaYv0"
   },
   "source": [
    "### QUS 19. How would you interpret a large standard error for a regression coefficient?\n",
    "- A large standard error means the model is unsure about the true value of the coefficient. It signals that the effect of that variable on the outcome is not estimated reliably and needs further investigation.\n",
    "- A large standard error suggests that the coefficient could vary widely if you were to repeat the study or resample the data.\n",
    "\n",
    "- It means there‚Äôs high uncertainty around the estimated effect of the predictor on the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvvDBZ9rfPTx"
   },
   "source": [
    "### QUS 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "\n",
    "- Plot Residuals vs. Fitted Values:\n",
    "\n",
    " - Create a scatter plot of the residuals on the y-axis and the fitted (predicted) values on the x-axis.\n",
    "\n",
    " - Look for Non-Constant Spread:If the residuals show a funnel shape (narrow at one end and wider at the other), this indicates increasing or decreasing variance ‚Äî a key sign of heteroscedasticity.\n",
    "   - Patterns like a fan, cone, or wave suggest that the variance of the residuals is not constant.\n",
    "- Why It‚Äôs Important to Address Heteroscedasticity:\n",
    " - Leads to Inefficient Estimates:While OLS coefficients remain unbiased, their standard errors become unreliable, making confidence intervals and hypothesis tests invalid.\n",
    "\n",
    " - Affects Inference:t-tests and F-tests may give incorrect conclusions due to underestimated or overestimated variances.\n",
    "\n",
    " - Makes unreliable models:Predictions and statistical inferences become less reliable, especially for observations with higher error variance.\n",
    "\n",
    " - Potential Indicator of Model Misfit:It may signal that important variables are missing, or that a transformation of variables (e.g., log) is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hhcUw4PhkIG"
   },
   "source": [
    "### QUS 21. What does it mean if a Multiple Linear Regression model has a high R2 but low adjusted R2?\n",
    "- Indicates that the model explains a large proportion of the variance in the dependent variable.\n",
    "- Adjusted R¬≤ penalizes the model for adding predictors that don't improve the model's explanatory power significantly.\n",
    "- If a Multiple Linear Regression model has a high R¬≤ but a low adjusted R¬≤, it suggests that some of the predictors in the model are not actually contributing meaningful information and may be overfitting the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISR-BQ7qnoxR"
   },
   "source": [
    "### QUS 22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "- Scaling variables in Multiple Linear Regression is important for several reasons, especially when the predictors have very different units or magnitudes. Here's why it matters:\n",
    "\n",
    "1. Improves Numerical Stability\n",
    "2. Makes Coefficients Comparable in Regularized Models\n",
    "3. Helps with Interpretation in Some Contexts\n",
    "4. Facilitates Detection of Multicollinearity.\n",
    "- Scaling helps ensure fair contribution of predictors, improves model stability, and is essential when using techniques like regularization or advanced preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n7k5SP0sI_6"
   },
   "source": [
    "### QUS 23. What is polynomial regression?\n",
    "- Polynomial regression is a type of regression analysis where the relationship between the independent variables and the dependent variable is modeled as an nth-degree polynomial.\n",
    "\n",
    "- Key Characteristics:\n",
    "  - It extends linear regression by allowing for curved relationships.\n",
    "\n",
    "  - The model still remains linear in terms of the coefficients, even though the predictors are raised to powers.\n",
    "- As the number of degree in polynomial regression increases the time complexity  and overfitting of model will also increase with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqVWuMeGt-On"
   },
   "source": [
    "### QUS 24. How does polynomial regression differ from linear regression?\n",
    "\n",
    " 1. Nature of the Relationship\n",
    "\n",
    "  Linear regression fits only linear (straight-line) trends.\n",
    "\n",
    "   Polynomial regression can fit curves, making it suitable for nonlinear patterns.\n",
    "\n",
    "  2. Predictor Transformation:\n",
    "\n",
    "   In linear regression, predictors are used in their original form.\n",
    "\n",
    "   In polynomial regression, predictors are raised to powers (e.g., squared, cubed) to capture curvature.\n",
    "\n",
    "  3. Model Complexity\n",
    "\n",
    "    Linear regression is simpler and more interpretable.\n",
    "\n",
    "    Polynomial regression adds complexity and can overfit if the polynomial degree is too high.\n",
    "\n",
    "  4. Fitting Method\n",
    "\n",
    "  Both use Ordinary Least Squares (OLS) or other optimization techniques.\n",
    "\n",
    "  Despite the nonlinear terms, polynomial regression is still linear in parameters, so it's fit using linear regression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww-Kca8_vGfl"
   },
   "source": [
    "### QUS 25. When is polynomial regression used?\n",
    "- Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is nonlinear, but can be approximated by a polynomial function. It allows a linear model to fit curved trends in the data by adding powers of the predictors.\n",
    "- Common Situations Where Polynomial Regression Used:\n",
    "  - Curved Data Patterns\n",
    "  - Improving Fit Beyond Linear Regression\n",
    "  - Modeling Real-World Phenomena:\n",
    "  - Predictive Modeling\n",
    "  - Feature Engineering\n",
    "- Example Use Cases:\n",
    "\n",
    "  Modeling how engine efficiency changes with temperature.\n",
    "\n",
    "  Describing the trajectory of an object under gravity (quadratic relation).\n",
    "\n",
    "  Estimating costs that increase at an increasing rate with demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6RIEslMxm0N"
   },
   "source": [
    "### QUS 26. What is the general equation for polynomial regression?\n",
    "- The general equation for polynomial regression is:\n",
    "\n",
    "Y = b0 + b1X + b2X^2 + b3X^3 + ... + bnX^n + error\n",
    "\n",
    "Where:\n",
    "\n",
    "Y is the dependent variable\n",
    "\n",
    "X is the independent variable\n",
    "\n",
    "b0, b1, ..., bn are the regression coefficients\n",
    "\n",
    "X^2, X^3, ..., X^n are the polynomial terms of X up to the nth degree\n",
    "\n",
    "error is the residual or error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiIPx44Ox9lT"
   },
   "source": [
    "### QUS 27. Can polynomial regression be applied to multiple variables?\n",
    "- Yes, polynomial regression can be applied to multiple variables. This is called multivariate polynomial regression.\n",
    "\n",
    "  - Instead of just including powers of a single variable, the model includes:\n",
    "\n",
    "  - Powers of each independent variable (like X1^2, X2^2)\n",
    "\n",
    "  - Interaction terms between variables (like X1X2, X1^2X2, etc.)\n",
    "\n",
    "  - Example equation with two variables:\n",
    "  Y = b0 + b1X1 + b2X2 + b3X1^2 + b4X2^2 + b5X1X2 + error\n",
    "\n",
    "- Key Points:\n",
    "  - It captures nonlinear relationships and interactions between multiple predictors.\n",
    "\n",
    "  - The number of terms grows quickly with the degree and number of variables, which can increase the risk of overfitting.\n",
    "\n",
    "  - It‚Äôs important to standardize variables before applying high-degree polynomial terms to improve model stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA_UfwxXzB3R"
   },
   "source": [
    "### QUS 28. What are the limitations of polynomial regression?\n",
    "- Polynomial regression has several limitations that should be considered before applying it to real-world data:\n",
    "\n",
    "1. **Overfitting**:\n",
    "\n",
    "   * High-degree polynomials can fit the training data very closely, capturing noise rather than the underlying pattern.\n",
    "   * This leads to poor performance on new, unseen data.\n",
    "\n",
    "2. **Extrapolation Problems**:\n",
    "\n",
    "   * Predictions outside the range of the training data can be highly unreliable.\n",
    "   * Polynomial curves tend to oscillate and diverge dramatically at the boundaries.\n",
    "\n",
    "3. **Interpretability**:\n",
    "\n",
    "   * As the degree increases, the model becomes harder to interpret.\n",
    "   * Coefficients of higher-degree terms don't have clear, intuitive meanings.\n",
    "\n",
    "4. **Multicollinearity**:\n",
    "\n",
    "   * Polynomial terms (like X, X^2, X^3, etc.) are often highly correlated, which can inflate standard errors and make estimates unstable.\n",
    "\n",
    "5. **Computational Complexity**:\n",
    "\n",
    "   * For large datasets or many predictors, the number of polynomial and interaction terms increases rapidly, making the model slower and more complex.\n",
    "\n",
    "6. **Sensitivity to Outliers**:\n",
    "\n",
    "   * Polynomial regression is sensitive to outliers, which can heavily distort the curve due to the influence of high powers.\n",
    "\n",
    "7. **Need for Feature Scaling**:\n",
    "\n",
    "   * When high-degree terms are used, large numerical values can cause instability or numerical errors unless variables are scaled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvhSxIGXzcHi"
   },
   "source": [
    "### QUS 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "- - When selecting the degree of a polynomial in regression, it's important to balance **model complexity** and **fit quality** to avoid underfitting or overfitting. Here are key methods to evaluate model fit:\n",
    "\n",
    "1. **Cross-Validation**:\n",
    "\n",
    "   * Use **k-fold cross-validation** to evaluate how the model generalizes to unseen data.\n",
    "   * Compare validation errors (e.g., Mean Squared Error) across different polynomial degrees.\n",
    "   * Helps detect overfitting with high-degree polynomials.\n",
    "\n",
    "2. **Adjusted R-squared**:\n",
    "\n",
    "   * Unlike R-squared, this metric adjusts for the number of predictors.\n",
    "   * It increases only if the new polynomial term improves the model more than expected by chance.\n",
    "   * A peak or drop in adjusted R-squared indicates the optimal complexity.\n",
    "\n",
    "3. **Residual Plots**:\n",
    "\n",
    "   * Plot residuals vs. predicted values to detect patterns.\n",
    "   * A good model should show residuals randomly scattered around zero.\n",
    "   * Patterns or curves in residuals suggest underfitting; erratic shapes may signal overfitting.\n",
    "\n",
    "4. **RMSE or MAE**:\n",
    "\n",
    "   * Lower values indicate better predictive accuracy.\n",
    "   * Compare these metrics on both training and validation sets to assess generalization.\n",
    "\n",
    "5. **Regularization (Ridge/Lasso)**:\n",
    "\n",
    "   * These can be applied in combination with polynomial features to control overfitting.\n",
    "   * They shrink less important coefficients, effectively simplifying the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzixTnLc0WET"
   },
   "source": [
    "### QUS 30. Why is visualization important in polynomial regression?\n",
    "- Visualization is important in polynomial regression because it helps you understand how well the model fits the data and whether it captures the underlying patterns appropriately. Here's why it's valuable:\n",
    "\n",
    "1. **Reveals the Shape of the Relationship**:\n",
    "2. **Detects Overfitting or Underfitting**:\n",
    "3. **Assesses Model Performance Intuitively**:\n",
    "4. **Checks Residual Patterns**:\n",
    "5. **Guides Degree Selection**:\n",
    "6. **Supports Communication and Reporting**:\n",
    "-  Visualization in polynomial regression is essential for diagnosing model fit, interpreting results, and making informed modeling decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAtPFG7b2A59"
   },
   "source": [
    "### QUS 31. How is polynomial regression implemented in Python?\n",
    "-\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([1, 4, 9, 16, 25, 36])\n",
    "# Create polynomial features (e.g., degree 2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "y_pred = model.predict(X_poly)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vO07aA33Q3f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPNv/2rqJ92ogSPrKDqgwr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
